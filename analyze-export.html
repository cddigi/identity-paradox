<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exported Video Analysis</title>
    <style>
        body {
            font-family: 'Courier New', monospace;
            background-color: #0a0a0a;
            color: #00CED1;
            margin: 20px;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
        }
        .section {
            margin: 20px 0;
            padding: 20px;
            border: 1px solid #00CED1;
            border-radius: 8px;
        }
        button {
            background-color: #00CED1;
            color: #0a0a0a;
            border: none;
            padding: 10px 20px;
            margin: 5px;
            cursor: pointer;
            border-radius: 4px;
            font-weight: bold;
        }
        button:hover {
            background-color: #00FFFF;
        }
        .video-analysis {
            display: flex;
            gap: 20px;
            margin: 20px 0;
        }
        .video-box {
            flex: 1;
        }
        video, canvas {
            width: 100%;
            max-width: 600px;
            border: 2px solid #00CED1;
            background-color: #000;
        }
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .metric-card {
            padding: 15px;
            background-color: rgba(0, 206, 209, 0.1);
            border-radius: 8px;
            text-align: center;
            border: 1px solid rgba(0, 206, 209, 0.3);
        }
        .metric-value {
            font-size: 24px;
            font-weight: bold;
            color: #00FFFF;
            margin: 10px 0;
        }
        .metric-label {
            font-size: 14px;
            color: #00CED1;
        }
        .analysis-log {
            height: 300px;
            overflow-y: auto;
            background-color: rgba(0, 0, 0, 0.5);
            padding: 15px;
            border: 1px solid #00CED1;
            border-radius: 4px;
            font-size: 12px;
            line-height: 1.4;
        }
        .log-entry {
            margin: 3px 0;
            padding: 2px 0;
        }
        .log-info { color: #00CED1; }
        .log-success { color: #00FF00; }
        .log-warning { color: #FFD700; }
        .log-error { color: #FF0000; }
        .progress-bar {
            width: 100%;
            height: 20px;
            background-color: rgba(0, 0, 0, 0.5);
            border: 1px solid #00CED1;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #00CED1, #00FFFF);
            width: 0%;
            transition: width 0.3s ease;
        }
        .frame-analysis {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin: 20px 0;
        }
        .frame-thumbnail {
            width: 120px;
            height: 80px;
            border: 2px solid #00CED1;
            background-color: #000;
            position: relative;
            cursor: pointer;
        }
        .frame-info {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            background: rgba(0, 0, 0, 0.8);
            color: #00CED1;
            font-size: 10px;
            padding: 2px 4px;
            text-align: center;
        }
        .report-section {
            background-color: rgba(0, 206, 209, 0.05);
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üîç Exported Video Analysis Tool</h1>
        
        <div class="section">
            <h2>Load Exported Video</h2>
            <input type="file" id="videoInput" accept="video/*" style="margin: 10px 0;" onchange="autoLoadVideo()">
            <div id="loadStatus" style="margin: 10px 0; color: #FFD700;">üìÅ Select a video file to begin analysis</div>
            <button onclick="startAnalysis()" id="analyzeBtn" disabled>Start Analysis</button>
            <button onclick="generateReport()" id="reportBtn" disabled>Generate Report</button>
        </div>

        <div class="section">
            <h2>Analysis Progress</h2>
            <div class="progress-bar">
                <div class="progress-fill" id="progressFill"></div>
            </div>
            <div id="progressText">Ready to analyze video...</div>
        </div>

        <div class="section">
            <h2>Video Analysis</h2>
            <div class="video-analysis">
                <div class="video-box">
                    <h3>Original Exported Video</h3>
                    <video id="analysisVideo" controls></video>
                </div>
                <div class="video-box">
                    <h3>Frame Analysis Canvas</h3>
                    <canvas id="analysisCanvas"></canvas>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Detection Metrics</h2>
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="metric-label">Total Frames</div>
                    <div class="metric-value" id="totalFrames">0</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Frames with Faces</div>
                    <div class="metric-value" id="framesWithFaces">0</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Total Face Detections</div>
                    <div class="metric-value" id="totalDetections">0</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Avg Faces per Frame</div>
                    <div class="metric-value" id="avgFaces">0.0</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Detection Accuracy</div>
                    <div class="metric-value" id="accuracy">0%</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Overlay Quality</div>
                    <div class="metric-value" id="overlayQuality">0%</div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Frame-by-Frame Analysis</h2>
            <div class="frame-analysis" id="frameAnalysis"></div>
        </div>

        <div class="section">
            <h2>Analysis Log</h2>
            <div class="analysis-log" id="analysisLog"></div>
        </div>

        <div class="section report-section" id="reportSection" style="display: none;">
            <h2>üìä Analysis Report</h2>
            <div id="reportContent"></div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <script>
        // Global variables
        let video, canvas, ctx;
        let analysisData = {
            totalFrames: 0,
            framesWithFaces: 0,
            totalDetections: 0,
            frameData: [],
            detectionAccuracy: 0,
            overlayQuality: 0
        };
        let modelsLoaded = false;

        // Logging function
        function log(message, type = 'info') {
            const logDiv = document.getElementById('analysisLog');
            const timestamp = new Date().toLocaleTimeString();
            const entry = document.createElement('div');
            entry.className = `log-entry log-${type}`;
            entry.textContent = `[${timestamp}] ${message}`;
            logDiv.appendChild(entry);
            logDiv.scrollTop = logDiv.scrollHeight;
        }

        // Update progress
        function updateProgress(percent, text) {
            document.getElementById('progressFill').style.width = `${percent}%`;
            document.getElementById('progressText').textContent = text;
        }

        // Load and initialize face detection models
        async function loadFaceModels() {
            if (modelsLoaded) return true;
            
            try {
                log('Loading face detection models...', 'info');
                const MODEL_URL = 'https://justadudewhohacks.github.io/face-api.js/models';
                await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
                modelsLoaded = true;
                log('Face detection models loaded successfully', 'success');
                return true;
            } catch (error) {
                log(`Failed to load models: ${error.message}`, 'error');
                return false;
            }
        }

        // Auto-load video when file is selected
        async function autoLoadVideo() {
            const input = document.getElementById('videoInput');
            const file = input.files[0];
            
            if (!file) {
                return;
            }

            document.getElementById('loadStatus').textContent = 'üîÑ Loading video and models...';
            
            try {
                video = document.getElementById('analysisVideo');
                canvas = document.getElementById('analysisCanvas');
                ctx = canvas.getContext('2d');

                const url = URL.createObjectURL(file);
                video.src = url;

                await new Promise((resolve, reject) => {
                    video.onloadedmetadata = () => {
                        canvas.width = video.videoWidth;
                        canvas.height = video.videoHeight;
                        log(`Video loaded: ${video.videoWidth}x${video.videoHeight}, duration: ${video.duration.toFixed(2)}s`, 'success');
                        resolve();
                    };
                    video.onerror = () => reject(new Error('Failed to load video'));
                });
                
                // Load face detection models
                document.getElementById('loadStatus').textContent = 'üîÑ Loading face detection models...';
                const modelsReady = await loadFaceModels();
                
                if (modelsReady) {
                    document.getElementById('analyzeBtn').disabled = false;
                    document.getElementById('loadStatus').innerHTML = '‚úÖ <span style="color: #00FF00;">Ready to analyze!</span> Click "Start Analysis" to begin.';
                    log('Video and models loaded - Ready to analyze', 'success');
                } else {
                    throw new Error('Failed to load face detection models');
                }
                
            } catch (error) {
                log(`Error loading video: ${error.message}`, 'error');
                document.getElementById('loadStatus').innerHTML = '‚ùå <span style="color: #FF0000;">Loading failed</span> - Try a different video file';
            }
        }

        // Legacy function for compatibility
        function loadVideo() {
            autoLoadVideo();
        }

        // Detect Laughing Man overlays in frame
        function detectLaughingManOverlays(imageData) {
            const data = imageData.data;
            const width = imageData.width;
            const height = imageData.height;
            
            let overlayPixels = 0;
            let totalPixels = width * height;
            
            // Look for characteristic Laughing Man colors (cyan/blue tones)
            for (let i = 0; i < data.length; i += 4) {
                const r = data[i];
                const g = data[i + 1];
                const b = data[i + 2];
                const a = data[i + 3];
                
                // Check for cyan/blue colors typical of Laughing Man overlay
                if (a > 200 && ((g > 180 && b > 180 && r < 100) || (g > 200 && b > 200))) {
                    overlayPixels++;
                }
            }
            
            return (overlayPixels / totalPixels) * 100;
        }

        // Analyze frame at specific time
        async function analyzeFrameAtTime(timepoint, frameIndex) {
            return new Promise(async (resolve) => {
                video.currentTime = timepoint;
                
                video.onseeked = async () => {
                    try {
                        // Draw frame to canvas
                        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                        
                        // Get image data for overlay analysis
                        const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                        const overlayPercentage = detectLaughingManOverlays(imageData);
                        
                        // Detect faces in the frame
                        const detections = await faceapi.detectAllFaces(
                            video,
                            new faceapi.TinyFaceDetectorOptions({
                                inputSize: 416,
                                scoreThreshold: 0.3
                            })
                        );
                        
                        const frameData = {
                            frameNumber: frameIndex,
                            timestamp: timepoint,
                            faceCount: detections.length,
                            overlayPercentage,
                            hasOverlay: overlayPercentage > 1.0,
                            detections: detections.map(d => ({
                                confidence: d.score,
                                box: d.box
                            }))
                        };
                        
                        analysisData.frameData.push(frameData);
                        
                        if (detections.length > 0) {
                            analysisData.framesWithFaces++;
                            analysisData.totalDetections += detections.length;
                        }
                        
                        // Create frame thumbnail
                        createFrameThumbnail(frameIndex, frameData);
                        
                        log(`Time ${timepoint.toFixed(1)}s: ${detections.length} faces, ${overlayPercentage.toFixed(1)}% overlay`, 'info');
                        
                        resolve();
                    } catch (error) {
                        log(`Error analyzing frame at ${timepoint.toFixed(1)}s: ${error.message}`, 'error');
                        resolve();
                    }
                };
            });
        }

        // Analyze single frame
        async function analyzeFrame(frameNumber, totalFrames) {
            return new Promise(async (resolve) => {
                video.currentTime = (frameNumber / totalFrames) * video.duration;
                
                video.onseeked = async () => {
                    try {
                        // Draw frame to canvas
                        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                        
                        // Get image data for overlay analysis
                        const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                        const overlayPercentage = detectLaughingManOverlays(imageData);
                        
                        // Detect faces in the frame
                        const detections = await faceapi.detectAllFaces(
                            video,
                            new faceapi.TinyFaceDetectorOptions({
                                inputSize: 416,
                                scoreThreshold: 0.3
                            })
                        );
                        
                        const frameData = {
                            frameNumber,
                            timestamp: video.currentTime,
                            faceCount: detections.length,
                            overlayPercentage,
                            hasOverlay: overlayPercentage > 1.0, // Threshold for overlay detection
                            detections: detections.map(d => ({
                                confidence: d.score,
                                box: d.box
                            }))
                        };
                        
                        analysisData.frameData.push(frameData);
                        
                        if (detections.length > 0) {
                            analysisData.framesWithFaces++;
                            analysisData.totalDetections += detections.length;
                        }
                        
                        // Create frame thumbnail
                        createFrameThumbnail(frameNumber, frameData);
                        
                        log(`Frame ${frameNumber}: ${detections.length} faces, ${overlayPercentage.toFixed(1)}% overlay`, 'info');
                        
                        resolve();
                    } catch (error) {
                        log(`Error analyzing frame ${frameNumber}: ${error.message}`, 'error');
                        resolve();
                    }
                };
            });
        }

        // Create frame thumbnail
        function createFrameThumbnail(frameNumber, frameData) {
            const thumbnailCanvas = document.createElement('canvas');
            thumbnailCanvas.width = 120;
            thumbnailCanvas.height = 80;
            thumbnailCanvas.className = 'frame-thumbnail';
            
            const thumbCtx = thumbnailCanvas.getContext('2d');
            thumbCtx.drawImage(video, 0, 0, 120, 80);
            
            const frameInfo = document.createElement('div');
            frameInfo.className = 'frame-info';
            frameInfo.textContent = `#${frameNumber} | ${frameData.faceCount}F | ${frameData.overlayPercentage.toFixed(1)}%`;
            
            const container = document.createElement('div');
            container.style.position = 'relative';
            container.style.display = 'inline-block';
            container.appendChild(thumbnailCanvas);
            container.appendChild(frameInfo);
            
            container.onclick = () => {
                video.currentTime = frameData.timestamp;
                log(`Jumped to frame ${frameNumber} at ${frameData.timestamp.toFixed(2)}s`, 'info');
            };
            
            document.getElementById('frameAnalysis').appendChild(container);
        }

        // Start analysis
        async function startAnalysis() {
            if (!video || !modelsLoaded) {
                log('Video or models not ready', 'error');
                return;
            }

            log('Starting comprehensive video analysis...', 'info');
            updateProgress(0, 'Initializing analysis...');
            
            // Reset analysis data
            analysisData = {
                totalFrames: 0,
                framesWithFaces: 0,
                totalDetections: 0,
                frameData: [],
                detectionAccuracy: 0,
                overlayQuality: 0
            };
            
            // Clear previous frame analysis
            document.getElementById('frameAnalysis').innerHTML = '';
            
            // Calculate total frames to analyze (sample every 10th frame for performance)
            const duration = video.duration;
            
            // Validate duration and limit analysis
            if (!duration || !isFinite(duration) || duration <= 0 || duration > 300) {
                log(`Invalid video duration: ${duration}. Using limited sampling instead.`, 'warning');
                // Use fixed sampling for problematic videos
                const maxSamples = 20;
                const sampleInterval = 1.0; // Sample every 1 second
                const framesToAnalyze = maxSamples;
                analysisData.totalFrames = framesToAnalyze;
                log(`Using fixed sampling: ${framesToAnalyze} samples over ${maxSamples} seconds`, 'info');
                
                // Limited time-based analysis
                for (let i = 0; i < framesToAnalyze; i++) {
                    const timepoint = i * sampleInterval;
                    await analyzeFrameAtTime(timepoint, i);
                    
                    const progress = ((i + 1) / framesToAnalyze) * 100;
                    updateProgress(progress, `Analyzing timepoint ${timepoint.toFixed(1)}s (${progress.toFixed(1)}%)`);
                    updateMetrics();
                }
                
                calculateFinalMetrics();
                updateMetrics();
                updateProgress(100, 'Analysis complete!');
                log('Video analysis completed successfully', 'success');
                document.getElementById('reportBtn').disabled = false;
                return;
            }
            
            // For valid short duration videos
            if (duration > 60) {
                log(`Long video detected (${duration.toFixed(1)}s). Limiting analysis to first 30 seconds.`, 'warning');
                duration = 30;
            }
            
            // Use time-based sampling for valid durations
            const sampleInterval = 0.5; // Sample every 0.5 seconds
            const framesToAnalyze = Math.floor(duration / sampleInterval);
            analysisData.totalFrames = framesToAnalyze;
            log(`Using time-based sampling: ${framesToAnalyze} samples over ${duration.toFixed(1)}s`, 'info');
            
            // Time-based analysis for valid duration
            for (let i = 0; i < framesToAnalyze; i++) {
                const timepoint = i * sampleInterval;
                await analyzeFrameAtTime(timepoint, i);
                
                const progress = ((i + 1) / framesToAnalyze) * 100;
                updateProgress(progress, `Analyzing timepoint ${timepoint.toFixed(1)}s (${progress.toFixed(1)}%)`);
                updateMetrics();
            }
            
            calculateFinalMetrics();
            updateMetrics();
            updateProgress(100, 'Analysis complete!');
            log('Video analysis completed successfully', 'success');
            document.getElementById('reportBtn').disabled = false;
        }

        // Calculate final metrics
        function calculateFinalMetrics() {
            const totalFrames = analysisData.frameData.length;
            const framesWithOverlays = analysisData.frameData.filter(f => f.hasOverlay).length;
            const framesWithFaces = analysisData.framesWithFaces;
            
            // Detection accuracy: percentage of frames with faces that also have overlays
            analysisData.detectionAccuracy = framesWithFaces > 0 ? 
                (framesWithOverlays / framesWithFaces) * 100 : 0;
            
            // Overlay quality: average overlay percentage for frames with overlays
            const overlayFrames = analysisData.frameData.filter(f => f.hasOverlay);
            analysisData.overlayQuality = overlayFrames.length > 0 ?
                overlayFrames.reduce((sum, f) => sum + f.overlayPercentage, 0) / overlayFrames.length : 0;
        }

        // Update metrics display
        function updateMetrics() {
            document.getElementById('totalFrames').textContent = analysisData.frameData.length;
            document.getElementById('framesWithFaces').textContent = analysisData.framesWithFaces;
            document.getElementById('totalDetections').textContent = analysisData.totalDetections;
            
            const avgFaces = analysisData.frameData.length > 0 ?
                analysisData.totalDetections / analysisData.frameData.length : 0;
            document.getElementById('avgFaces').textContent = avgFaces.toFixed(1);
            
            document.getElementById('accuracy').textContent = `${analysisData.detectionAccuracy.toFixed(1)}%`;
            document.getElementById('overlayQuality').textContent = `${analysisData.overlayQuality.toFixed(1)}%`;
        }

        // Generate detailed report
        function generateReport() {
            const reportSection = document.getElementById('reportSection');
            const reportContent = document.getElementById('reportContent');
            
            const report = `
                <h3>üéØ Detection Performance</h3>
                <ul>
                    <li><strong>Overall Accuracy:</strong> ${analysisData.detectionAccuracy.toFixed(1)}% of frames with faces have Laughing Man overlays</li>
                    <li><strong>Coverage:</strong> ${((analysisData.framesWithFaces / analysisData.frameData.length) * 100).toFixed(1)}% of frames contain detected faces</li>
                    <li><strong>Average Faces per Frame:</strong> ${(analysisData.totalDetections / analysisData.frameData.length).toFixed(2)}</li>
                </ul>
                
                <h3>üé® Overlay Quality</h3>
                <ul>
                    <li><strong>Average Overlay Coverage:</strong> ${analysisData.overlayQuality.toFixed(1)}% of frame pixels</li>
                    <li><strong>Frames with Overlays:</strong> ${analysisData.frameData.filter(f => f.hasOverlay).length} / ${analysisData.frameData.length}</li>
                    <li><strong>Overlay Consistency:</strong> ${analysisData.frameData.filter(f => f.hasOverlay && f.faceCount > 0).length} overlays correctly placed on faces</li>
                </ul>
                
                <h3>üìà Frame Analysis Summary</h3>
                <ul>
                    <li><strong>Total Frames Analyzed:</strong> ${analysisData.frameData.length}</li>
                    <li><strong>Frames with Face Detection:</strong> ${analysisData.framesWithFaces}</li>
                    <li><strong>Frames with Overlay:</strong> ${analysisData.frameData.filter(f => f.hasOverlay).length}</li>
                    <li><strong>Perfect Matches:</strong> ${analysisData.frameData.filter(f => f.hasOverlay && f.faceCount > 0).length} (overlay + face detection)</li>
                </ul>
                
                <h3>üîç Quality Assessment</h3>
                <p><strong>Test Result:</strong> ${getQualityAssessment()}</p>
            `;
            
            reportContent.innerHTML = report;
            reportSection.style.display = 'block';
            
            log('Analysis report generated', 'success');
        }

        // Get quality assessment
        function getQualityAssessment() {
            const accuracy = analysisData.detectionAccuracy;
            const coverage = (analysisData.framesWithFaces / analysisData.frameData.length) * 100;
            
            if (accuracy >= 90 && coverage >= 80) {
                return 'üü¢ EXCELLENT - High accuracy face detection with consistent overlay placement';
            } else if (accuracy >= 70 && coverage >= 60) {
                return 'üü° GOOD - Acceptable performance with some missed detections';
            } else if (accuracy >= 50 || coverage >= 40) {
                return 'üü† FAIR - Moderate performance, may need optimization';
            } else {
                return 'üî¥ POOR - Low detection rate, requires investigation';
            }
        }

        // Initialize
        window.onload = function() {
            log('Video analysis tool ready', 'info');
            updateProgress(0, 'Select a video file to begin analysis');
        };
    </script>
</body>
</html>